# Welcome to 20250818 AI Report

- We-Math 2.0: 该系统旨在提升多模态大型语言模型在数学推理方面的能力，集成了结构化知识体系和强化学习训练框架。通过构建MathBook知识系统和MathBook-RL强化学习方法，实验结果显示其在数学推理任务中表现优异。

- NextStep-1: 这是一个140亿参数的自回归模型，专注于文本到图像生成，采用离散文本和连续图像标记进行训练。该模型在图像生成和编辑任务中展现出强大的性能，并计划向社区发布代码和模型以促进研究。

- ToonComposer: 该生成模型将卡通制作中的补间和上色过程统一为后关键帧阶段，减少了人工工作量。通过稀疏草图注入机制和空间低秩适配器，ToonComposer在视觉质量和生产效率上优于现有方法。

- PRELUDE: 这是一个评估长文本理解能力的基准，要求判断角色前传故事与原著叙述的一致性。实验表明，当前模型在这一任务上表现不佳，强调了长文本理解和推理的改进空间。

- UI-Venus Technical Report: 该用户界面代理通过截图作为输入，在定位和导航任务上实现了最佳性能。引入了强化微调和自我演变方法，提升了导航能力，并发布了开源代码供社区使用。

- Puppeteer: 这是一个自动绑定和动画制作的框架，旨在简化3D内容创作过程。通过自回归变换器和基于注意力的架构，Puppeteer在骨骼预测和动画质量上显著优于现有技术。

- STream3R: 该方法将3D重建问题重新定义为仅使用解码器的Transformer问题，利用因果注意力高效处理图像序列。实验结果显示其在动态场景中的表现优于传统方法，并兼容大型语言模型的训练基础设施。

- A Survey on Diffusion Language Models: 本调查提供了扩散语言模型的全面概述，分析了其与自回归模型的关系及应用。强调了DLM在多模态扩展和实际场景中的潜力，同时讨论了其局限性和未来研究方向。

- Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models: 该研究探讨了在强化学习中使用Pass@k作为奖励的优势，强调探索与利用的相互促进。通过解析推导，提出了优化策略设计的新视角。

- HumanSense: 这是一个评估多模态大型语言模型人本感知和互动能力的基准，强调对复杂人类意图的理解。研究表明，结合视觉和音频输入可以显著提升模型表现，并提出了增强推理能力的方法。

- Processing and acquisition traces in visual encoders: 该研究分析了图像获取过程中的参数如何影响视觉编码器的性能。发现这些参数在学习的视觉表征中被编码，并对语义预测有深远影响。

- From Black Box to Transparency: 该研究提出了一种多维建模框架，结合可解释的机器学习和特征工程，提升自动化口译质量评估的透明性。结果显示该方法在评估忠实度和流畅度方面表现优异。

- When Explainability Meets Privacy: 该研究探讨了自然语言处理中的隐私与可解释性之间的权衡，揭示了两者之间复杂的关系。总结了一系列实用建议，以指导未来在这一交叉领域的研究。

- Archon: 这是一个为AI编码助手提供知识和任务管理的核心支持平台，旨在提升开发效率。用户可以通过简单的设置连接多个AI工具，利用智能搜索和任务管理功能。

- Parlant: 该框架通过自然语言定义规则，解决AI代理开发中的一致性问题。开发者可以快速启动代理，并确保其在实际应用中满足用户需求，适用于多个行业。

- Data Engineer Handbook: 这是一个数据工程学习资源的汇总库，提供了入门路线图、实践案例和社区支持。项目旨在帮助用户提升数据工程技能，了解行业动态。

- LLMs from Scratch: 该项目提供了从零开始使用PyTorch实现大型语言模型的代码和教程，适合希望深入了解LLM的学习者。书中详细讲解了模型构建过程，并提供了练习和额外材料。

- Awesome n8n Templates: 该项目提供了一系列n8n自动化模板，旨在提升工作流程的自动化效率。用户可以快速连接常用应用程序，利用现成的自动化解决方案。

- PixiEditor: 这是一个通用的2D编辑器，支持用户创建游戏精灵、动画和图像设计。软件提供多种工具集，用户可以在同一画布上灵活使用，支持多种格式导出。

- Immich: 该项目是一个高性能的自托管照片和视频管理解决方案，支持多种功能如自动备份和元数据搜索。用户需遵循数据安全策略，并可参与项目的进一步发展。

- Awesome LLM Apps: 该项目汇集了众多大型语言模型应用，展示了LLM在各个领域的实际应用。鼓励用户探索和贡献新想法，推动开源生态系统的发展。

- Motia: 这是一个现代化的后端框架，旨在简化后端开发过程。通过统一的系统，开发者可以灵活使用多种语言，提升开发效率和系统可维护性。

- OpenBB: 该平台是一个开源金融数据聚合工具，提供丰富的金融数据访问。用户可以通过简单的命令获取数据，并参与项目的改进和发展。

## Introduction

We-Math 2.0: A Versatile MathBook System for Incentivizing Visual   Mathematical Reasoning

多模态大型语言模型（MLLMs）在多项任务中展现了出色的能力，但在复杂数学推理方面仍存在不足。现有研究主要集中于数据集构建和方法优化，往往忽视了两个关键方面：全面的知识驱动设计和以模型为中心的数据空间建模。为此，本文提出了We-Math 2.0，这是一个统一系统，集成了结构化的数学知识体系、以模型为中心的数据空间建模以及基于强化学习的训练范式，以全面提升MLLMs的数学推理能力。

We-Math 2.0的主要贡献包括四个方面：首先，构建了一个名为MathBook的知识系统，包含五个层级的491个知识点和1819条基本原则。其次，开发了MathBook-Standard数据集，确保广泛的概念覆盖和灵活性，并通过双重扩展实现。此外，定义了一个三维难度空间，为每个问题生成7个渐进变体，形成了MathBook-Pro，这是一个用于强健训练的挑战性数据集。第三，提出了MathBook-RL，一个两阶段的强化学习框架，包括冷启动微调，旨在将模型与知识导向的思维链推理对齐，以及渐进对齐强化学习，利用平均奖励学习和动态数据调度实现不同难度水平的渐进对齐。

最后，We-Math 2.0引入了MathBookEval，一个全面的基准测试，涵盖所有491个知识点，并具有多样的推理步骤分布。实验结果表明，MathBook-RL在四个广泛使用的基准测试中表现出色，并在MathBookEval上取得了强劲的结果，显示出在数学推理方面的良好泛化能力。


NextStep-1: Toward Autoregressive Image Generation with Continuous   Tokens at Scale

当前主流的自回归（AR）模型在文本到图像生成中，通常依赖于计算量大的扩散模型来处理连续的图像标记，或者使用向量量化（VQ）来获取离散标记，但这会导致量化损失。本文提出了NextStep-1，这是一个拥有140亿参数的自回归模型，配备了一个1.57亿参数的流匹配头，采用离散文本标记和连续图像标记进行训练，目标是下一个标记的预测。NextStep-1在文本到图像生成任务中实现了自回归模型的最先进性能，展现了在高保真图像合成方面的强大能力。

此外，我们的方法在图像编辑方面也表现出色，突显了我们统一方法的强大和多样性。为了促进开放研究，我们计划向社区发布我们的代码和模型，以便更多的研究者能够使用和改进这一技术。这一创新不仅推动了自回归模型的发展，也为文本到图像生成领域带来了新的可能性，展示了在生成和编辑图像方面的广泛应用潜力。


ToonComposer: Streamlining Cartoon Production with Generative   Post-Keyframing

传统的卡通和动漫制作过程包括关键帧、补间和上色等多个阶段，这些阶段通常需要大量的人工努力。尽管近年来人工智能技术有所进步，但现有的方法往往将这些阶段分开处理，导致错误累积和伪影的产生。例如，补间方法在处理大幅度运动时表现不佳，而上色方法则需要密集的逐帧草图。为了解决这些问题，我们提出了ToonComposer，这是一种将补间和上色统一为单一后关键帧阶段的生成模型。

ToonComposer采用稀疏草图注入机制，通过关键帧草图提供精确控制。此外，它还使用空间低秩适配器的卡通适应方法，将现代视频基础模型调整到卡通领域，同时保持其时间先验不变。ToonComposer只需一个草图和一个上色参考帧，就能在稀疏输入下表现出色，同时还支持在任何时间位置使用多个草图，以实现更精确的运动控制。这种双重能力减少了人工工作量，提高了灵活性，使艺术家在实际应用中更加得心应手。

为了评估我们的模型，我们进一步创建了PKBench，这是一个包含人手绘制草图的基准，模拟真实世界的使用场景。我们的评估结果表明，ToonComposer在视觉质量、运动一致性和生产效率方面优于现有方法，提供了一种更优越和灵活的AI辅助卡通制作解决方案。


PRELUDE: A Benchmark Designed to Require Global Comprehension and   Reasoning over Long Contexts

我们介绍了PRELUDE，这是一个用于评估长文本理解能力的基准，任务是判断一个角色的前传故事是否与原著的叙述一致。与现有基准相比，这一任务对整体理解和深度推理的要求更高，因为前传并不是原故事的一部分，评估其合理性通常需要搜索和整合间接相关的信息。实证研究表明，88%的案例需要从叙述的多个部分获取证据。

实验结果突显了这一任务的挑战性：在上下文学习、检索增强生成（RAG）和使用最先进的大型语言模型（LLM）进行领域内训练，以及商业深度研究服务的表现上，模型的表现普遍落后于人类超过15%。进一步的人类研究显示，尽管模型经常能给出正确答案，但其推理过程存在缺陷，导致推理准确性与人类相比存在超过30%的差距。这些发现强调了在长文本理解和推理方面仍有很大的改进空间。


UI-Venus Technical Report: Building High-performance UI Agents with RFT

我们介绍了UI-Venus，这是一种基于多模态大语言模型的原生用户界面代理，能够仅通过截图作为输入。UI-Venus在用户界面定位和导航任务上实现了当前的最佳性能，使用了仅数十万个高质量的训练样本，并通过基于Qwen2.5-VL的强化微调（RFT）进行优化。具体而言，UI-Venus的7B和72B版本在标准定位基准测试Screenspot-V2和Pro上分别获得了94.1%和50.8%、95.3%和61.9%的成绩，超越了包括开源的GTA1和闭源的UI-TARS-1.5在内的先前最佳基线。

为了展示UI-Venus的总结和规划能力，我们还在AndroidWorld这一在线用户界面导航平台上进行了评估，结果显示7B和72B版本的成功率分别为49.1%和65.9%，同样优于现有模型。为实现这一成果，我们引入了精心设计的奖励函数，针对用户界面定位和导航任务，并制定了相应的高效数据清理策略。为了进一步提升导航性能，我们提出了自我演变轨迹历史对齐与稀疏动作增强的方法，这一方法能够优化历史推理轨迹，并平衡稀疏但关键动作的分布，从而实现更连贯的规划和在复杂用户界面任务中的更好泛化能力。

我们的贡献包括发布当前最佳的开源用户界面代理、全面的数据清理协议以及一种新颖的自我演变框架，以提升导航性能，这些都为社区的进一步研究和开发提供了支持。相关代码可在https://github.com/inclusionAI/UI-Venus获取。


Puppeteer: Rig and Animate Your 3D Models

现代互动应用对动态3D内容的需求日益增加，但将静态3D模型转化为动画资产的过程在内容创作中仍然是一个重要瓶颈。尽管最近生成性人工智能在静态3D模型创建方面取得了突破，但绑定和动画制作仍然高度依赖专家的干预。为此，我们提出了Puppeteer，这是一个全面的框架，旨在实现多样化3D对象的自动绑定和动画。

该系统首先通过自回归变换器预测合理的骨骼结构，采用基于关节的标记策略以实现紧凑表示，并结合层次化排序方法和随机扰动，增强双向学习能力。接着，它利用基于注意力的架构推断皮肤权重，该架构结合了拓扑感知的关节注意力，明确编码了基于骨骼图距离的关节间关系。最后，我们通过可微优化的动画管道补充了这些绑定进展，生成稳定且高保真的动画，同时在计算效率上优于现有方法。

在多个基准测试中的广泛评估表明，我们的方法在骨骼预测准确性和皮肤质量方面显著优于当前最先进的技术。该系统能够稳健地处理多样化的3D内容，从专业设计的游戏资产到AI生成的形状，生成的动画时间上连贯，消除了现有方法中常见的抖动问题。


STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer

我们提出了STream3R，这是一种新颖的3D重建方法，将点图预测重新定义为仅使用解码器的Transformer问题。现有的多视角重建最先进方法通常依赖于昂贵的全局优化，或是依赖于简单的记忆机制，这些机制在序列长度增加时表现不佳。与此不同，STream3R引入了一种流式框架，利用因果注意力高效处理图像序列，这一思路受到现代语言建模进展的启发。通过从大规模3D数据集中学习几何先验，STream3R在多样化和具有挑战性的场景中表现出良好的泛化能力，尤其是在动态场景中，传统方法往往难以应对。

大量实验表明，我们的方法在静态和动态场景基准测试中均持续超越以往的研究成果。此外，STream3R与大型语言模型（LLM）风格的训练基础设施天然兼容，使得在各种下游3D任务中实现高效的大规模预训练和微调成为可能。我们的结果强调了因果Transformer模型在在线3D感知中的潜力，为流媒体环境中的实时3D理解铺平了道路。更多细节可以在我们的项目页面找到。


A Survey on Diffusion Language Models

扩散语言模型（DLMs）正在迅速崛起，成为一种强大且有前景的替代方案，取代了主流的自回归（AR）模型。DLMs通过迭代去噪过程并行生成标记，具有减少推理延迟和捕捉双向上下文的固有优势，从而实现对生成过程的细粒度控制。近期的进展使得DLMs在速度上实现了数倍的提升，同时在性能上与自回归模型相当，使其成为各种自然语言处理任务的有力选择。

本调查提供了当前DLM领域的全面概述，追溯其发展历程及与其他模型（如自回归和掩蔽语言模型）的关系，涵盖了基础原理和最先进的模型。我们的研究提供了最新的分类法，并深入分析了当前技术，从预训练策略到先进的后训练方法。此外，我们还对DLM推理策略和优化进行了详尽的回顾，包括解码并行性、缓存机制和生成质量的改进。

我们还强调了DLM在多模态扩展方面的最新方法，并阐明了其在各种实际场景中的应用。同时，讨论中也提及了DLMs的局限性和挑战，包括效率、长序列处理和基础设施需求，并概述了未来的研究方向，以推动这一快速发展的领域的进步。


Pass@k Training for Adaptively Balancing Exploration and Exploitation of   Large Reasoning Models

强化学习中的可验证奖励（RLVR）通常采用Pass@1作为奖励，但在探索与利用之间的平衡上存在问题，导致策略倾向于保守行为，从而收敛到局部最优解。因此，确定合适的奖励指标显得尤为重要。尽管以往的研究中使用了Pass@k进行评估，但其与RLVR中大语言模型（LLM）探索能力的关系却未得到充分关注。为此，我们首先使用Pass@k作为奖励来训练策略模型（即“Pass@k训练”），并观察其在探索能力上的提升。

接下来，我们推导出Pass@k训练的优势的解析解，从而形成一个高效且有效的过程。我们的分析表明，探索与利用并非固有的对立目标，反而可以相互促进。此外，基于解析推导的Pass@k训练本质上涉及直接设计优势函数。受到这一发现的启发，我们初步探索了RLVR中的优势设计，显示出良好的结果，并强调了未来研究的潜在方向。这些研究不仅为RLVR提供了新的视角，也为优化策略的设计提供了理论基础，推动了强化学习领域的进一步发展。


HumanSense: From Multimodal Perception to Empathetic Context-Aware   Responses through Reasoning MLLMs

多模态大型语言模型（MLLMs）在实现人类般的互动方面展现出巨大潜力，但由于缺乏针对人本场景的细致评估框架，进展受到限制。这些框架需要涵盖对复杂人类意图的理解以及提供富有同情心和上下文意识的回应。为此，我们提出了HumanSense，这是一个全面的基准，旨在评估MLLMs的人本感知和互动能力，特别关注对扩展多模态上下文的深刻理解和合理反馈的形成。

我们的评估结果显示，领先的MLLMs在高级互动任务上仍有相当大的改进空间。通过将视觉输入与音频和文本信息相结合，可以显著提升模型的表现，而全模态模型在这些任务上表现出优势。此外，我们认为，适当的反馈源于对对话者需求和情感的上下文分析，而推理能力则是解锁这一反馈的关键。因此，我们采用多阶段、模态渐进的强化学习方法来增强全模态模型的推理能力，从而在评估结果上取得显著提升。

我们还观察到，成功的推理过程展现出高度一致的思维模式。通过设计相应的提示，我们也在无训练的情况下提升了非推理模型的表现。这些研究成果为进一步推动多模态大型语言模型的应用和发展提供了重要的理论基础和实践指导。


Processing and acquisition traces in visual encoders: What does CLIP   know about your camera?

以往的研究主要集中在视觉编码器对图像变换和损坏的鲁棒性，尤其是在训练过程中未见过的情况下。这种情况会在测试时引入分布偏移，通常导致性能下降。研究的重点多集中在严重的损坏上，这些损坏在强烈应用时会扭曲进行准确语义预测所需的有用信号。然而，我们采取了不同的视角，分析了图像获取过程和变换的参数，这些参数可能是微妙的，甚至对人眼不可察觉。我们的研究发现，这些参数在学习的视觉表征中被系统性地编码，并且可以轻松恢复。

更引人注目的是，这些参数的存在对语义预测有着深远的影响，可能是积极的也可能是消极的。这种影响取决于语义标签与这些基于获取或处理的标签之间是否存在强相关性或反相关性。通过这种分析，我们揭示了图像获取过程中的细微变化如何影响视觉编码器的性能，强调了在实际应用中考虑这些因素的重要性。我们的代码和数据已公开，供研究者进一步探索这一领域。


From Black Box to Transparency: Enhancing Automated Interpreting   Assessment with Explainable AI in College Classrooms

近年来，机器学习的进步引发了对自动化口译质量评估的关注。然而，现有研究在语言使用质量的检验上存在不足，模型效果因数据稀缺和不平衡而不尽如人意，同时对模型预测的解释性也缺乏足够的关注。为了解决这些问题，我们提出了一种多维建模框架，结合了特征工程、数据增强和可解释的机器学习。该方法优先考虑可解释性，避免“黑箱”预测，采用与构建相关的透明特征，并进行Shapley值（SHAP）分析。

我们的研究结果在一个新的英汉连续口译数据集上显示出强大的预测性能，发现BLEURT和CometKiwi评分是评估忠实度的最强预测特征，而与停顿相关的特征则与流畅度密切相关，中文特有的短语多样性指标则用于语言使用的评估。总体而言，通过强调可解释性，我们提供了一种可扩展、可靠且透明的替代传统人工评估的方法。这种方法不仅能够为学习者提供详细的诊断反馈，还支持自我调节学习的优势，而这些是单纯依赖自动评分所无法实现的。


When Explainability Meets Privacy: An Investigation at the Intersection   of Post-hoc Explainability and Differential Privacy in the Context of Natural   Language Processing

在可信自然语言处理（NLP）的研究中，解释性和隐私保护成为了两个重要的研究领域。尽管近年来对可解释性和隐私保护的NLP研究兴趣显著增加，但两者交叉领域的研究仍然不足。这使得我们对实现可解释性与隐私保护是否可能存在较大空白，或者两者是否存在矛盾的理解不够深入。本研究通过实证调查，探讨了NLP中的隐私与可解释性之间的权衡，主要依托于差分隐私（DP）和事后解释性等流行方法。

我们的研究结果揭示了隐私与可解释性之间复杂的关系，这种关系受到多个因素的影响，包括下游任务的性质以及文本隐私化和解释性方法的选择。在这一过程中，我们强调了隐私与可解释性共存的潜力，并总结出一系列实用建议，以指导未来在这一重要交叉领域的研究工作。这些建议旨在帮助研究者更好地理解和应对隐私与可解释性之间的挑战，从而推动NLP技术的可信性和实用性发展。


https://github.com/coleam00/Archon

Archon OS目前处于测试阶段，旨在为AI编码助手提供知识和任务管理的核心支持。作为一个命令中心，Archon为用户提供了一个简洁的界面，以管理项目的知识、上下文和任务，同时为AI助手提供了一个模型上下文协议（MCP）服务器，使其能够共享相同的知识和任务。用户可以将多个AI工具连接到Archon，利用其智能搜索、任务管理和实时更新等功能，从而提升AI驱动编码的效率。

Archon的设计理念是替代之前的agenteer，成为一个更强大的工具。无论是新项目还是现有代码库，Archon的知识和任务管理能力都能显著改善输出效果。用户可以通过GitHub讨论区参与交流，贡献代码或反馈问题，帮助团队不断改进Archon。

在快速启动方面，用户需要安装Docker和Supabase，并获取OpenAI API密钥。设置过程包括克隆代码库、配置环境变量、设置数据库以及启动服务。Archon的架构采用微服务结构，确保各个服务之间的独立性和灵活性，支持实时协作和多用户操作。

Archon的核心功能包括智能网页爬虫、文档处理、代码示例提取和向量搜索等知识管理工具，以及与AI助手的无缝集成。项目和任务管理功能允许用户以层次化的方式组织工作，并通过AI助手生成项目需求和任务。此外，Archon还支持实时更新和健康监测，确保系统的稳定性和高效性。

总之，Archon OS为AI编码助手提供了一个强大的知识和任务管理平台，旨在通过集成多种功能和服务，提升开发者的工作效率和协作体验。


https://github.com/emcie-co/parlant

Parlant是一种新型的AI代理开发框架，旨在解决开发者在构建生产级AI代理时常遇到的问题，如系统提示被忽视、关键时刻的错误响应以及无法一致处理边缘案例等。传统的AI开发方法往往依赖复杂的系统提示和不确定的行为，而Parlant则通过自然语言定义规则，确保代理遵循这些规则，从而实现可预测和一致的行为。

使用Parlant，开发者可以在短短60秒内启动代理。通过简单的代码，用户可以定义代理的行为和响应，确保其在实际应用中能够有效地满足用户需求。Parlant的设计理念是将规则的定义与自然语言结合，使得开发过程更加直观和高效。

该框架特别适用于金融服务、医疗保健、电子商务和法律科技等领域，具备合规性设计、风险管理、客户服务自动化等功能。Parlant还提供了一系列企业级特性，如对话旅程引导、动态规则匹配、可靠的工具集成和深入的对话分析，帮助开发者不断优化代理的响应。

目前，已有超过5000名开发者在使用Parlant，许多金融机构、医疗提供者和法律公司等企业已将其应用于生产环境。开发者们对Parlant的评价普遍积极，认为其是一个优雅且高效的对话AI框架，极大地简化了开发流程。

总之，Parlant为AI代理的开发提供了一种全新的解决方案，确保代理能够在真实环境中稳定运行，满足用户的实际需求。


https://github.com/DataExpert-io/data-engineer-handbook

该项目是一个数据工程学习资源的汇总库，旨在帮助用户成为优秀的数据工程师。对于初学者，建议从2024年数据工程入门路线图开始学习，此外还提供了为期六周的免费YouTube训练营，涵盖了软件需求、项目实践、面试技巧等内容。用户可以通过项目部分获取更多实践案例，通过面试部分获取面试建议，通过书籍部分找到高质量的参考书籍，并加入相关社区以获取更多支持。

资源部分列出了超过25本书籍，其中推荐的三本书包括《数据工程基础》、《设计数据密集型应用》和《设计机器学习系统》。此外，还推荐了10个数据工程和机器学习相关的社区，供用户加入以拓展人脉和获取信息。项目还列出了多家相关公司的信息，涵盖了数据湖、数据仓库、数据质量等多个领域，帮助用户了解行业现状和技术趋势。

在博客、白皮书和社交媒体方面，项目提供了多家公司的数据工程博客链接、重要的白皮书以及数据工程领域的社交媒体账号，便于用户获取最新的行业动态和技术分享。项目还推荐了一些优秀的播客和通讯，帮助用户通过不同的媒介持续学习。

最后，项目还提供了课程和认证信息，涵盖了多个在线学习平台和认证课程，帮助用户在数据工程领域获得专业资格和技能提升。整体而言，该项目为希望进入数据工程领域的学习者提供了全面的资源和支持。


https://github.com/rasbt/LLMs-from-scratch

该项目旨在从零开始使用PyTorch实现一个类似ChatGPT的大型语言模型（LLM），并提供了相关的代码和教程。该代码库是书籍《从零开始构建大型语言模型》的官方代码库，书中详细讲解了大型语言模型的工作原理，逐步指导读者创建自己的LLM，涵盖了每个阶段的清晰文本、图示和示例。书中所描述的训练和开发方法与创建大型基础模型的方式相似，同时也提供了加载更大预训练模型权重以进行微调的代码。

读者需要具备扎实的Python编程基础，了解深度神经网络的概念将有助于理解LLM的构建。书中使用PyTorch实现代码，尽管不要求精通PyTorch，但熟悉其基本知识会有所帮助。书中还提供了附录，介绍了PyTorch的基础知识。代码设计为能够在普通笔记本电脑上运行，并能自动利用可用的GPU，确保广泛的受众能够参与学习。

此外，书中每章都包含多个练习，解决方案汇总在附录中，读者还可以下载免费的170页PDF文件进行自测。项目还提供了额外的材料和视频课程，帮助读者更深入地理解内容。对于有兴趣的读者，项目欢迎反馈和讨论，但由于代码与书籍内容紧密相关，暂不接受扩展内容的贡献。

如果读者在研究中发现该书或代码有用，建议引用该书籍。整体而言，该项目为希望深入了解和构建大型语言模型的学习者提供了全面的资源和指导。


https://github.com/enescingoz/awesome-n8n-templates

该项目旨在通过提供一系列n8n自动化模板来提升工作流程的自动化效率。这些模板可以快速连接用户常用的应用程序，如Gmail、Telegram、Google Drive和Slack，利用人工智能技术实现即用型自动化，帮助用户节省时间、提高生产力。项目的文档中包含了从互联网收集的多种n8n自动化模板，旨在简化用户的任务和工作流程，使其更容易发现和使用现成的自动化解决方案。

项目中包含的模板涵盖了多个领域，包括社交媒体自动化、电子邮件管理、数据处理和分析等。例如，用户可以使用模板自动翻译和发布Twitter线程，或通过Gmail自动标记和分类邮件。此外，项目还提供了与Telegram和Google Drive的集成，用户可以通过AI助手与这些平台进行交互，提升工作效率。

需要注意的是，所有模板均为在线收集，项目作者不对使用过程中可能出现的问题或损失承担责任。用户在使用这些模板时应自行承担风险。项目鼓励用户参与贡献新的模板或建议新类别，以进一步丰富资源库。

总之，该项目为希望通过自动化提升工作效率的用户提供了丰富的工具和资源，用户只需简单几步即可开始使用n8n进行自动化操作。


https://github.com/PixiEditor/PixiEditor

PixiEditor是一款通用的2D编辑器，旨在为用户提供满足各种2D需求的工具和功能。用户可以利用PixiEditor创建精美的游戏精灵、动画、编辑图像以及设计标志，所有这些功能都集成在一个直观且熟悉的界面中。该软件的2.0版本默认提供三种工具集，分别是像素艺术工具、基础绘画工具和矢量工具，用户可以在同一画布上混合使用这些工具，支持导出为多种格式，如png、jpg、svg、gif和mp4等。

在动画方面，PixiEditor 2.0引入了时间轴和动画功能，用户可以逐帧创建动画，或使用节点来动画自定义着色器。未来的版本计划支持关键帧动画与矢量结合。节点渲染系统是该软件强大功能的核心，所有图层、效果和层结构都以节点的形式存在，用户可以自由定制图像，创造程序化艺术和动画。

此外，PixiEditor还提供了从源代码构建的指南，鼓励用户参与贡献和协作。对于遇到问题的用户，PixiEditor团队也提供了帮助和支持。总之，PixiEditor是一个功能强大且灵活的2D编辑工具，适合各种创作需求。


https://github.com/immich-app/immich

该项目是一个高性能的自托管照片和视频管理解决方案，正在积极开发中，用户需注意可能存在的错误和重大变更。项目不建议作为唯一的照片和视频存储方式，用户应遵循3-2-1备份策略以确保数据安全。项目的主要文档和安装指南可在官方网站找到。

该解决方案支持多种语言，提供了丰富的功能，包括在移动端和网页上上传和查看视频及照片、自动备份、避免重复资产、选择性备份相册、下载本地设备、支持多用户、共享相册、支持原始格式、元数据查看、以及基于元数据和对象的搜索等。移动应用还具备后台备份、虚拟滚动、OAuth支持等功能。

用户可以通过提供的演示链接体验该应用，演示账户的登录信息也已公开。项目的文档中详细列出了功能特性，并提供了安装和使用的相关指导。用户还可以查看项目的开发路线图和贡献方式，参与到项目的进一步发展中。

此外，项目还具备面部识别、记忆功能、离线支持等先进特性，旨在为用户提供全面的照片和视频管理体验。整体而言，该项目为用户提供了一个灵活且功能强大的工具，适合个人和团队使用。


https://github.com/Shubhamsaboo/awesome-llm-apps

该项目是一个汇集了众多优秀大型语言模型（LLM）应用的集合，主要利用OpenAI、Anthropic、Gemini等模型以及开源模型如DeepSeek、Qwen和Llama，支持本地运行。项目旨在展示LLM在各个领域的实际应用，包括代码库、电子邮件等，鼓励用户探索结合AI代理、代理团队、MCP和RAG的应用。

项目中包含了多种AI代理的示例，包括初级和高级代理，如AI博客转播代理、AI数据分析代理、AI医疗影像代理等。此外，还有自主游戏代理、多个代理团队的协作应用，以及语音AI代理和MCP AI代理等多种类型的应用。RAG（检索增强生成）技术也被广泛应用于多个项目中，提供了多种检索和生成的组合方式。

用户可以通过克隆项目库、进入特定项目目录并安装所需依赖来快速上手。同时，项目鼓励社区贡献，欢迎用户提出新想法或改进建议，并通过GitHub提交问题或拉取请求。项目的目标是促进开源生态系统的发展，让更多人参与到LLM驱动的应用开发中。

总之，该项目不仅提供了丰富的LLM应用实例，还为开发者提供了学习和贡献的机会，推动了AI技术的普及与应用。


https://github.com/MotiaDev/motia

Motia是一个现代化的后端框架，旨在解决当前后端开发中的碎片化问题。传统的后端开发通常涉及多个框架和工具，例如API、后台作业、队列和AI代理等，导致开发过程复杂且难以管理。Motia通过将这些功能统一到一个系统中，提供了共享的可观察性和开发体验，使得后端开发变得更加简洁高效。开发者可以在同一个工作流中使用JavaScript、TypeScript和Python等多种语言，极大地简化了开发过程。

Motia的核心概念是“步骤”（Step），每个步骤代表一个独立的入口点，可以是API、后台作业、定时任务或AI代理。通过这种方式，开发者可以在同一个项目中灵活地使用不同的编程语言，同时共享状态和数据。Motia还提供了内置的可观察性工具，支持完整的端到端追踪、结构化日志记录和状态可视化，帮助开发者快速调试和监控应用。

使用Motia，开发者可以在不到60秒的时间内启动一个新项目，并通过简单的命令创建REST API、处理后台任务或调度定时作业。Motia的工作台提供了可视化的界面，方便开发者实时构建、测试和观察后端应用。该框架还支持事件驱动架构，允许开发者通过发出和订阅事件来构建复杂的工作流。

Motia的设计理念是消除后端开发中的复杂性，提供一个统一的运行时环境，使得开发者能够专注于业务逻辑而非基础设施。通过Motia，开发者可以实现快速开发、灵活扩展和简化部署，适应现代软件开发的需求。总之，Motia为后端开发提供了一种全新的解决方案，旨在提升开发效率和系统的可维护性。


https://github.com/OpenBB-finance/OpenBB

OpenBB平台是一个开源的金融数据聚合工具，旨在为用户和人工智能代理提供丰富的金融数据访问，包括股票、期权、加密货币、外汇、宏观经济和固定收益等。用户可以通过简单的Python命令安装和使用该平台，获取所需的金融数据。OpenBB还提供了多种扩展功能，以满足不同用户的需求。

该平台的接口支持Python和命令行界面（CLI），用户可以通过OpenBB工作区实现数据可视化和AI代理的集成。用户只需按照简单的步骤将OpenBB平台与工作区连接，即可开始使用。OpenBB工作区允许用户在一个集中的环境中管理和分析数据，提升用户体验。

安装OpenBB平台非常简单，用户可以通过PyPI包或直接克隆代码库来完成安装。此外，OpenBB还鼓励用户参与贡献，提供反馈或报告问题，以帮助项目不断改进。平台的使用涉及一定的风险，用户在进行金融交易前应充分了解相关风险和成本，并根据自身的投资目标和风险承受能力做出明智决策。

OpenBB平台在不断发展壮大，欢迎更多用户加入这个开源社区。用户可以通过社交媒体或电子邮件与团队联系，获取更多信息或寻求支持。总之，OpenBB致力于通过开放的金融数据平台，推动金融行业的创新与变革。


## Repo Trendings

### Repo: coleam00/Archon

url: https://github.com/coleam00/Archon

language: Python


Beta release of Archon OS - the knowledge and task management backbone for AI coding assistants.


### Repo: emcie-co/parlant

url: https://github.com/emcie-co/parlant

language: Python


LLM agents built for control. Designed for real-world use. Deployed in minutes.


### Repo: DataExpert-io/data-engineer-handbook

url: https://github.com/DataExpert-io/data-engineer-handbook

language: Jupyter Notebook


This is a repo with links to everything you'd ever want to learn about data engineering


### Repo: rasbt/LLMs-from-scratch

url: https://github.com/rasbt/LLMs-from-scratch

language: Jupyter Notebook


Implement a ChatGPT-like LLM in PyTorch from scratch, step by step


### Repo: enescingoz/awesome-n8n-templates

url: https://github.com/enescingoz/awesome-n8n-templates

language: Unknown


Supercharge your workflow automation with this curated collection of n8n templates! Instantly connect your favorite apps-like Gmail, Telegram, Google Drive, Slack, and more-with ready-to-use, AI-powered automations. Save time, boost productivity, and unlock the true potential of n8n in just a few clicks.


### Repo: PixiEditor/PixiEditor

url: https://github.com/PixiEditor/PixiEditor

language: C#


PixiEditor is a Universal Editor for all your 2D needs


### Repo: immich-app/immich

url: https://github.com/immich-app/immich

language: TypeScript


High performance self-hosted photo and video management solution.


### Repo: Shubhamsaboo/awesome-llm-apps

url: https://github.com/Shubhamsaboo/awesome-llm-apps

language: Python


Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.


### Repo: MotiaDev/motia

url: https://github.com/MotiaDev/motia

language: TypeScript


Modern Backend Framework that unifies APIs, background jobs, workflows, and AI agents into a single cohesive system with built-in observability and state management.


### Repo: OpenBB-finance/OpenBB

url: https://github.com/OpenBB-finance/OpenBB

language: Python


Financial data aggregator for humans and AI agents.


## Paper Trendings

### Paper: We-Math 2.0: A Versatile MathBook System for Incentivizing Visual   Mathematical Reasoning

url: http://arxiv.org/pdf/2508.10433v1


Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling. In this paper, we introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.


### Paper: NextStep-1: Toward Autoregressive Image Generation with Continuous   Tokens at Scale

url: http://arxiv.org/pdf/2508.10711v1


Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.


### Paper: ToonComposer: Streamlining Cartoon Production with Generative   Post-Keyframing

url: http://arxiv.org/pdf/2508.10881v1


Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production.


### Paper: PRELUDE: A Benchmark Designed to Require Global Comprehension and   Reasoning over Long Contexts

url: http://arxiv.org/pdf/2508.09848v2


We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning.


### Paper: UI-Venus Technical Report: Building High-performance UI Agents with RFT

url: http://arxiv.org/pdf/2508.10833v2


We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models. To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies. To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment & Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/inclusionAI/UI-Venus.


### Paper: Puppeteer: Rig and Animate Your 3D Models

url: http://arxiv.org/pdf/2508.10898v1


Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods.


### Paper: STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer

url: http://arxiv.org/pdf/2508.10893v1


We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r.


### Paper: A Survey on Diffusion Language Models

url: http://arxiv.org/pdf/2508.10875v1


Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.


### Paper: Pass@k Training for Adaptively Balancing Exploration and Exploitation of   Large Reasoning Models

url: http://arxiv.org/pdf/2508.10751v1


Reinforcement learning with verifiable rewards (RLVR), which typically adopts Pass@1 as the reward, has faced the issues in balancing exploration and exploitation, causing policies to prefer conservative actions, converging to a local optimum. Identifying an appropriate reward metric is therefore crucial. Regarding the prior work, although Pass@k has been used in evaluation, its connection to LLM exploration ability in RLVR remains largely overlooked. To investigate this, we first use Pass@k as the reward to train the policy model (i.e., $\textbf{Pass@k Training}$), and observe the improvement on its exploration ability. Next, we derive an analytical solution for the advantage of Pass@k Training, leading to an efficient and effective process. Building on this, our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. Moreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we preliminarily explore the advantage design for RLVR, showing promising results and highlighting a potential future direction.


### Paper: HumanSense: From Multimodal Perception to Empathetic Context-Aware   Responses through Reasoning MLLMs

url: http://arxiv.org/pdf/2508.10576v1


While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: \textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/


### Paper: Processing and acquisition traces in visual encoders: What does CLIP   know about your camera?

url: http://arxiv.org/pdf/2508.10637v1


Prior work has analyzed the robustness of visual encoders to image transformations and corruptions, particularly in cases where such alterations are not seen during training. When this occurs, they introduce a form of distribution shift at test time, often leading to performance degradation. The primary focus has been on severe corruptions that, when applied aggressively, distort useful signals necessary for accurate semantic predictions.   We take a different perspective by analyzing parameters of the image acquisition process and transformations that may be subtle or even imperceptible to the human eye. We find that such parameters are systematically encoded in the learned visual representations and can be easily recovered. More strikingly, their presence can have a profound impact, either positively or negatively, on semantic predictions. This effect depends on whether there is a strong correlation or anti-correlation between semantic labels and these acquisition-based or processing-based labels. Our code and data are available at: https://github.com/ryan-caesar-ramos/visual-encoder-traces


### Paper: From Black Box to Transparency: Enhancing Automated Interpreting   Assessment with Explainable AI in College Classrooms

url: http://arxiv.org/pdf/2508.10860v1


Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.


### Paper: When Explainability Meets Privacy: An Investigation at the Intersection   of Post-hoc Explainability and Differential Privacy in the Context of Natural   Language Processing

url: http://arxiv.org/pdf/2508.10482v2


In the study of trustworthy Natural Language Processing (NLP), a number of important research fields have emerged, including that of explainability and privacy. While research interest in both explainable and privacy-preserving NLP has increased considerably in recent years, there remains a lack of investigation at the intersection of the two. This leaves a considerable gap in understanding of whether achieving both explainability and privacy is possible, or whether the two are at odds with each other. In this work, we conduct an empirical investigation into the privacy-explainability trade-off in the context of NLP, guided by the popular overarching methods of Differential Privacy (DP) and Post-hoc Explainability. Our findings include a view into the intricate relationship between privacy and explainability, which is formed by a number of factors, including the nature of the downstream task and choice of the text privatization and explainability method. In this, we highlight the potential for privacy and explainability to co-exist, and we summarize our findings in a collection of practical recommendations for future work at this important intersection.


