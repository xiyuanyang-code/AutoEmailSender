# Welcome to 20250818 AI Report

- We-Math 2.0: 该系统整合了结构化数学知识、数据空间建模和强化学习训练，旨在提升多模态大型语言模型在数学推理方面的能力。它构建了一个包含491个知识点的知识体系，并开发了多个数据集和评估基准，实验结果显示其在数学推理上表现优异。
- NextStep-1: 这是一个新型自回归模型，结合离散文本标记和连续图像标记进行训练，旨在提升文本到图像生成的性能。该模型在图像生成和编辑任务中展现了强大的能力，并计划向社区发布代码和模型以促进研究。
- ToonComposer: 该生成模型将插帧和上色统一为后关键帧阶段，采用稀疏草图注入机制，显著减少了卡通制作中的人工工作量。评估结果表明其在视觉质量和生产效率上优于现有方法。
- PRELUDE: 这是一个用于评估长文本理解能力的基准，要求模型判断角色的前传故事与原著的一致性。实验显示，当前模型在此任务上的表现仍显不足，强调了长文本理解和推理的改进空间。
- UI-Venus Technical Report: 该用户界面代理通过截图作为输入，在定位和导航任务上实现了最佳性能。引入了强化微调和自我演变方法，提升了模型的规划能力和导航性能。
- Puppeteer: 这是一个自动绑定和动画制作的框架，旨在简化动态3D内容的创作过程。通过自回归变换器和基于注意力的架构，该系统在骨骼预测和动画生成方面表现优异。
- STream3R: 该3D重建方法利用因果Transformer处理图像序列，克服了传统方法的局限。实验结果显示其在静态和动态场景中的表现优于之前的技术，展现了实时3D理解的潜力。
- A Survey on Diffusion Language Models: 本调查全面回顾了扩散语言模型的演变及其与其他模型的关系，强调了其在自然语言处理中的应用潜力。讨论了DLM的局限性和未来研究方向。
- Pass@k Training for Adaptively Balancing Exploration and Exploitation of Large Reasoning Models: 该研究提出使用Pass@k作为奖励来训练策略模型，以提升探索能力。分析表明探索与利用可以相互促进，为优化奖励机制提供了新视角。
- HumanSense: 这是一个评估多模态大型语言模型人本感知和互动能力的基准，强调对复杂人类意图的理解。研究显示，结合视觉和音频输入可以显著提升模型表现。
- Processing and acquisition traces in visual encoders: 该研究分析了图像获取过程对视觉编码器性能的影响，揭示了其在细微变化下的脆弱性。强调了在图像处理和理解中考虑这些因素的重要性。
- From Black Box to Transparency: 该研究提出了一种结合可解释性和数据增强的自动化口译质量评估框架，强调了可解释性在模型预测中的重要性。结果显示该方法在新数据集上表现出色。
- When Explainability Meets Privacy: 本研究探讨了自然语言处理中的隐私与可解释性之间的关系，强调两者共存的潜力。提出了未来研究的建议，以促进这一交叉领域的探索。
- Archon: 这是一个AI编程助手的知识和任务管理基础架构，旨在提升开发效率。用户可以通过简单的界面管理项目知识，并连接多种AI工具以优化编码过程。
- Parlant: 该AI代理开发框架解决了传统代理在实际使用中的问题，提供了可靠的自然语言规则遵循。开发者可以快速启动代理并定义其行为，适用于多个行业。
- Data Engineer Handbook: 这是一个数据工程学习资源的汇总库，提供了入门路线图、书籍推荐和社区资源，帮助用户成为优秀的数据工程师。项目还包含多个在线课程和认证链接。
- LLMs from Scratch: 该项目提供了从零开始使用PyTorch实现大型语言模型的详细步骤和代码，适合教育目的。用户可以通过书籍和视频课程深入理解LLM的构建过程。
- Awesome n8n Templates: 该项目提供了一系列n8n自动化模板，旨在提升工作流程的自动化效率。用户可以快速连接常用应用程序，节省时间并提高生产力。
- PixiEditor: 这是一个通用的2D编辑器，支持创建游戏精灵、动画和图像编辑。用户可以在直观界面中使用多种工具，满足各种2D创作需求。
- Immich: 这是一个高性能的自托管照片和视频管理解决方案，提供多种功能以便捷管理多媒体内容。用户需注意数据安全和备份策略。
- Awesome LLM Apps: 该项目汇集了众多大型语言模型应用，展示了其在不同领域的实际和创造性应用。用户可以探索如何将这些模型与AI代理结合使用。
- Motia: 这是一个现代化的后端框架，旨在简化后端开发过程。通过统一的系统，开发者可以在同一工作流中使用多种语言，提高开发效率。
- OpenBB: 该平台是一个开源金融数据聚合器，提供多种金融数据访问。用户可以通过简单的命令获取数据，并鼓励社区参与和反馈。

## Introduction

We-Math 2.0: A Versatile MathBook System for Incentivizing Visual   Mathematical Reasoning

多模态大型语言模型（MLLMs）在多项任务中展现了出色的能力，但在复杂数学推理方面仍存在困难。现有研究主要集中在数据集构建和方法优化上，往往忽视了两个关键方面：全面的知识驱动设计和以模型为中心的数据空间建模。为此，本文提出了We-Math 2.0，一个统一系统，整合了结构化的数学知识体系、以模型为中心的数据空间建模以及基于强化学习的训练范式，以全面提升MLLMs的数学推理能力。

We-Math 2.0的主要贡献包括四个方面：首先，构建了一个名为MathBook的知识体系，包含五个层级的491个知识点和1819个基本原则。其次，开发了MathBook-Standard数据集，确保广泛的概念覆盖和灵活性，并定义了一个三维难度空间，为每个问题生成7个渐进变体，形成MathBook-Pro，这是一个用于强健训练的挑战性数据集。第三，提出了MathBook-RL，一个两阶段的强化学习框架，包括冷启动微调和渐进对齐强化学习，前者使模型与知识导向的推理相一致，后者则利用平均奖励学习和动态数据调度实现不同难度水平的渐进对齐。

最后，We-Math 2.0还引入了MathBookEval，一个全面的基准测试，覆盖所有491个知识点，并具有多样的推理步骤分布。实验结果表明，MathBook-RL在四个广泛使用的基准测试中表现出色，并在MathBookEval上取得了强劲的结果，显示出其在数学推理方面的良好泛化能力。


NextStep-1: Toward Autoregressive Image Generation with Continuous   Tokens at Scale

当前主流的自回归（AR）模型在文本到图像生成中，通常依赖于计算密集型的扩散模型来处理连续的图像标记，或者使用向量量化（VQ）来获取离散标记，但这会导致量化损失。本文提出了一种新的自回归模型NextStep-1，该模型拥有140亿参数，并配备了一个1.57亿参数的流匹配头，采用离散文本标记和连续图像标记进行训练，目标是进行下一个标记的预测。NextStep-1在文本到图像生成任务中实现了自回归模型的最先进性能，展现了在高保真图像合成方面的强大能力。

此外，我们的方法在图像编辑方面也表现出色，突显了我们统一方法的强大和多样性。为了促进开放研究，我们计划向社区发布我们的代码和模型，以便更多的研究人员能够使用和改进这一技术。这一研究不仅推动了自回归模型的发展，也为文本到图像生成领域带来了新的可能性，展示了在图像生成和编辑任务中的广泛应用潜力。


ToonComposer: Streamlining Cartoon Production with Generative   Post-Keyframing

传统的卡通和动漫制作过程包括关键帧、插帧和上色等多个阶段，这些阶段通常需要大量的人工努力。尽管近年来人工智能技术有所进步，但现有的方法往往将这些阶段分开处理，导致错误累积和伪影的产生。例如，插帧方法在处理大幅度运动时表现不佳，而上色方法则需要密集的逐帧草图。为了解决这些问题，我们提出了ToonComposer，这是一种将插帧和上色统一为单一后关键帧阶段的生成模型。

ToonComposer采用稀疏草图注入机制，通过关键帧草图提供精确控制。此外，它使用空间低秩适配器的卡通适应方法，将现代视频基础模型调整到卡通领域，同时保持其时间先验不变。ToonComposer只需一个草图和一个上色参考帧，就能在稀疏输入下表现出色，同时也支持在任何时间位置使用多个草图，以实现更精确的运动控制。这种双重能力减少了人工工作量，提高了灵活性，赋予艺术家在实际场景中的更多创作自由。

为了评估我们的模型，我们还创建了PKBench，这是一个包含人手绘制草图的基准，模拟真实的使用案例。我们的评估结果表明，ToonComposer在视觉质量、运动一致性和生产效率方面优于现有方法，提供了一种更优越且灵活的AI辅助卡通制作解决方案。


PRELUDE: A Benchmark Designed to Require Global Comprehension and   Reasoning over Long Contexts

我们介绍了PRELUDE，这是一个用于评估长文本理解能力的基准，任务是判断一个角色的前传故事是否与原著的叙述一致。与现有基准相比，这一任务对整体理解和深度推理的要求更高，因为前传并不是原故事的一部分，评估其合理性通常需要查找和整合间接相关的信息。实证研究表明，88%的案例需要从叙述的多个部分获取证据。

实验结果突显了这一任务的挑战性：在上下文学习、检索增强生成（RAG）和使用最先进的大型语言模型（LLM）及商业深度研究服务的情况下，模型的表现比人类低了超过15%。进一步的人类研究显示，尽管模型经常给出正确答案，但推理过程存在缺陷，导致其推理准确率与人类相比存在超过30%的差距。这些发现强调了在长文本理解和推理方面仍有很大的改进空间。


UI-Venus Technical Report: Building High-performance UI Agents with RFT

我们介绍了UI-Venus，这是一种基于多模态大语言模型的原生用户界面代理，仅通过截图作为输入。UI-Venus在用户界面定位和导航任务上实现了当前的最佳性能，使用了仅数十万高质量的训练样本，并通过基于Qwen2.5-VL的强化微调（RFT）进行训练。具体而言，UI-Venus的7B和72B版本在标准定位基准测试Screenspot-V2和Pro上分别取得了94.1%和50.8%、95.3%和61.9%的成绩，超越了包括开源的GTA1和闭源的UI-TARS-1.5在内的先前最佳基线。

为了展示UI-Venus的总结和规划能力，我们还在AndroidWorld这一在线用户界面导航平台上进行了评估，结果显示7B和72B版本分别达到了49.1%和65.9%的成功率，同样超越了现有模型。为实现这一目标，我们引入了精心设计的奖励函数，针对用户界面定位和导航任务，并制定了相应的高效数据清理策略。为了进一步提升导航性能，我们提出了自我演变轨迹历史对齐与稀疏动作增强的方法，这一方法能够优化历史推理轨迹，并平衡稀疏但关键动作的分布，从而实现更连贯的规划和在复杂用户界面任务中的更好泛化能力。

我们的贡献包括发布当前最佳的开源用户界面代理、全面的数据清理协议以及一种新颖的自我演变框架，以提升导航性能，这些都将促进社区内的进一步研究和开发。相关代码可在https://github.com/inclusionAI/UI-Venus获取。


Puppeteer: Rig and Animate Your 3D Models

现代互动应用对动态3D内容的需求日益增加，但将静态3D模型转化为动画资产的过程在内容创作中仍然是一个重要瓶颈。尽管近年来生成性人工智能在静态3D模型创建方面取得了突破，但绑定和动画制作仍然高度依赖专家的干预。为了解决这一问题，我们提出了Puppeteer，这是一个全面的框架，旨在实现多样化3D对象的自动绑定和动画。

该系统首先通过自回归变换器预测合理的骨骼结构，采用基于关节的标记策略以实现紧凑表示，并结合分层排序方法和随机扰动，增强双向学习能力。接着，它通过一种基于注意力的架构推断皮肤权重，该架构结合了拓扑感知的关节注意力，明确编码了基于骨骼图距离的关节间关系。最后，我们在这些绑定进展的基础上，补充了一个基于可微优化的动画管道，能够生成稳定且高保真的动画，同时在计算效率上优于现有方法。

通过在多个基准测试中的广泛评估，我们的方法在骨骼预测准确性和皮肤质量方面显著优于当前最先进的技术。该系统能够稳健地处理多样化的3D内容，从专业设计的游戏资产到AI生成的形状，生成的动画在时间上保持一致，消除了现有方法中常见的抖动问题。


STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer

我们提出了STream3R，这是一种新颖的3D重建方法，将点图预测重新构建为仅使用解码器的Transformer问题。现有的多视角重建最先进方法通常依赖于昂贵的全局优化，或者依赖于简单的内存机制，这在序列长度增加时表现不佳。与此不同，STream3R引入了一种流式框架，通过因果注意力高效处理图像序列，这一灵感来源于现代语言建模的进展。通过从大规模3D数据集中学习几何先验，STream3R能够很好地推广到多样化和具有挑战性的场景，包括传统方法常常失效的动态场景。

大量实验表明，我们的方法在静态和动态场景基准测试中始终优于之前的工作。此外，STream3R与大语言模型（LLM）风格的训练基础设施天然兼容，使得在各种下游3D任务中实现高效的大规模预训练和微调成为可能。我们的结果强调了因果Transformer模型在在线3D感知中的潜力，为流媒体环境中的实时3D理解铺平了道路。更多详细信息可以在我们的项目页面找到。


A Survey on Diffusion Language Models

扩散语言模型（DLMs）正迅速崛起，成为一种强大且有前景的替代方案，取代了主流的自回归（AR）模型。DLMs通过迭代去噪过程并行生成标记，具备降低推理延迟和捕捉双向上下文的固有优势，从而实现对生成过程的细粒度控制。近期的进展使得DLMs在速度上实现了数倍的提升，同时在性能上与自回归模型相当，使其在各种自然语言处理任务中成为一个引人注目的选择。

本调查提供了当前DLM领域的全面概述，追溯了其演变及与其他模型（如自回归和掩蔽语言模型）的关系，涵盖了基础原理和最先进的模型。我们的工作提供了最新的综合分类法，并对当前技术进行了深入分析，从预训练策略到先进的后训练方法，均有涉及。此外，本调查还对DLM推理策略和优化进行了详尽的回顾，包括解码并行性、缓存机制和生成质量的改进。

我们还强调了DLM的多模态扩展的最新方法，并阐明了其在各种实际场景中的应用。同时，讨论中也提及了DLM的局限性和挑战，包括效率、长序列处理和基础设施需求，并概述了未来研究方向，以支持这一快速发展的领域的进步。


Pass@k Training for Adaptively Balancing Exploration and Exploitation of   Large Reasoning Models

强化学习中的可验证奖励（RLVR）通常采用Pass@1作为奖励，但在探索与利用之间的平衡上存在问题，导致策略倾向于保守行为，从而收敛到局部最优解。因此，识别合适的奖励指标显得尤为重要。尽管以往的研究中使用了Pass@k进行评估，但其与RLVR中大语言模型（LLM）探索能力的关系仍然被忽视。为此，我们首先使用Pass@k作为奖励来训练策略模型（即“Pass@k训练”），并观察其在探索能力上的提升。

接下来，我们推导出Pass@k训练的优势的解析解，从而形成一个高效且有效的过程。我们的分析表明，探索与利用并不是固有的对立目标，反而可以相互促进。此外，基于解析推导的Pass@k训练本质上涉及直接设计优势函数。受到这一发现的启发，我们初步探索了RLVR中的优势设计，显示出良好的结果，并强调了未来可能的研究方向。这些研究不仅为RLVR提供了新的视角，也为优化奖励机制和提升模型性能开辟了新的路径。


HumanSense: From Multimodal Perception to Empathetic Context-Aware   Responses through Reasoning MLLMs

多模态大型语言模型（MLLMs）在实现人类般的互动方面展现出巨大潜力，但由于缺乏针对人本场景的细致评估框架，进展受到限制。这些框架需要涵盖对复杂人类意图的理解以及提供富有同理心和上下文意识的回应。为此，我们提出了HumanSense，这是一个综合性基准，旨在评估MLLMs的人本感知和互动能力，特别关注对扩展多模态上下文的深刻理解和合理反馈的形成。

我们的评估结果显示，领先的MLLMs在高级互动任务上仍有相当大的改进空间。通过将视觉输入与音频和文本信息相结合，可以显著提升模型的表现，而全模态模型在这些任务中表现出优势。此外，我们认为，适当的反馈源于对对话者需求和情感的上下文分析，而推理能力是实现这一目标的关键。因此，我们采用了多阶段、模态渐进的强化学习方法，以增强全模态模型的推理能力，从而在评估结果上取得显著提升。

我们还观察到，成功的推理过程展现出高度一致的思维模式。通过设计相应的提示，我们也在无训练的情况下提升了非推理模型的表现。这些研究成果为进一步推动多模态大型语言模型在复杂人类互动中的应用提供了重要的理论和实践基础。


Processing and acquisition traces in visual encoders: What does CLIP   know about your camera?

以往的研究主要关注视觉编码器在图像变换和损坏下的鲁棒性，尤其是在训练过程中未见过的变化。这种情况在测试时引入了分布偏移，通常导致性能下降。研究的重点多集中在严重的损坏上，这些损坏在强烈应用时会扭曲准确语义预测所需的有用信号。然而，我们采取了不同的视角，分析了图像获取过程和变换的参数，这些参数可能是微妙的，甚至对人眼不可察觉。我们的研究发现，这些参数在学习到的视觉表征中被系统性地编码，并且可以轻松恢复。

更引人注目的是，这些参数的存在对语义预测有着深远的影响，可能是积极的也可能是消极的。这种影响取决于语义标签与这些基于获取或处理的标签之间的强相关性或反相关性。通过这种分析，我们揭示了视觉编码器在面对细微变化时的潜在脆弱性和敏感性，强调了在图像处理和理解中考虑这些因素的重要性。我们的代码和数据已公开，供研究者进一步探索这一领域。


From Black Box to Transparency: Enhancing Automated Interpreting   Assessment with Explainable AI in College Classrooms

近年来，机器学习的进步引发了对自动化口译质量评估的关注。然而，现有研究在语言使用质量的检验上存在不足，模型效果因数据稀缺和不平衡而不尽如人意，同时缺乏对模型预测结果的解释。为了解决这些问题，我们提出了一种多维建模框架，结合了特征工程、数据增强和可解释的机器学习。该方法优先考虑可解释性，避免“黑箱”预测，采用与构建相关的透明特征，并进行Shapley值（SHAP）分析。

我们的研究结果显示，在一个新的英汉连续口译数据集上，模型具有强大的预测性能。我们发现，BLEURT和CometKiwi得分是评估忠实度的最强预测特征，而与停顿相关的特征则对流畅度的评估最为有效。此外，针对中文的短语多样性指标在语言使用方面也表现出显著的预测能力。总体而言，通过强调可解释性，我们提供了一种可扩展、可靠且透明的替代方案，取代传统的人类评估方法。这种方法不仅能够为学习者提供详细的诊断反馈，还支持自我调节学习的优势，而这些是单靠自动评分无法实现的。


When Explainability Meets Privacy: An Investigation at the Intersection   of Post-hoc Explainability and Differential Privacy in the Context of Natural   Language Processing

在可信自然语言处理（NLP）的研究中，解释性和隐私保护成为了两个重要的研究领域。尽管近年来对可解释性和隐私保护的NLP研究兴趣显著增加，但两者交叉领域的研究仍然不足。这导致我们对实现可解释性与隐私保护是否可能存在较大空白，或者两者是否存在矛盾的理解不够深入。本研究通过实证调查，探讨了NLP中隐私与可解释性之间的权衡，主要依托于差分隐私（DP）和后期解释性等流行方法。

我们的研究揭示了隐私与可解释性之间复杂的关系，这种关系受到多个因素的影响，包括下游任务的性质以及文本隐私化和解释性方法的选择。我们强调了隐私和可解释性共存的潜力，并总结了我们的发现，提出了一系列针对未来研究的实用建议。这些建议旨在促进在隐私和可解释性交叉领域的进一步探索，帮助研究者更好地理解如何在确保隐私的同时实现模型的可解释性，从而推动NLP技术的可信应用。


https://github.com/coleam00/Archon

Archon OS目前处于测试阶段，旨在为AI编程助手提供知识和任务管理的基础架构。作为一个命令中心，Archon为用户提供了一个简洁的界面，以管理项目的知识、上下文和任务，同时为AI助手提供了一个模型上下文协议（MCP）服务器，以便它们能够共享相同的知识和任务。用户可以将多种AI工具连接到Archon，利用其智能搜索、任务管理和实时更新等功能，提升AI驱动编码的效率。

Archon的设计理念是替代之前的agenteer，成为一个更强大的工具，支持用户在任何代码基础上进行开发。它的知识管理功能包括智能网页爬虫、文档处理和向量搜索，能够高效地提取和组织信息。此外，Archon还支持多种AI模型的集成，提供实时流式响应和多用户协作功能，确保团队能够高效地进行项目管理。

用户在使用Archon时需要准备Docker和Supabase账户，并按照说明进行环境配置和服务启动。系统的微服务架构确保了各个组件的独立性和可扩展性，用户可以根据需要自定义端口和主机名。Archon的开发者社区鼓励用户参与反馈和贡献，致力于不断改进这一工具。

总之，Archon OS为AI编程助手提供了一个集成的知识和任务管理环境，旨在提升开发效率和协作能力，适用于各种项目和代码基础。


https://github.com/emcie-co/parlant

Parlant是一种新型的AI代理开发框架，旨在解决开发者在构建生产级AI代理时常遇到的问题。传统的AI代理在测试中表现良好，但在实际使用中往往会出现忽视系统提示、产生错误响应和无法一致处理边缘案例等问题。Parlant通过确保代理遵循自然语言定义的规则，提供了一种更可靠的解决方案，避免了开发者在复杂系统提示和不确定行为之间的挣扎。

使用Parlant，开发者可以在短短60秒内启动代理。通过简单的代码，用户可以定义代理的行为和响应，确保其在特定条件下执行预定的操作。这种方法不仅提高了代理的可靠性，还使得开发过程更加直观和高效。Parlant的设计适用于多个行业，包括金融服务、医疗保健、电子商务和法律科技，能够满足各类合规和风险管理需求。

Parlant的企业级功能包括对话旅程引导、动态规则匹配、可靠的工具集成和深入的对话分析等。这些功能使得开发者能够创建更具交互性和智能化的AI代理，提升用户体验。此外，Parlant还提供了内置的保护机制，以防止代理产生不相关或错误的响应。

目前，已有超过5000名开发者在使用Parlant，许多金融机构、医疗服务提供商和电商平台都在其生产环境中应用这一框架。开发者们对Parlant的评价普遍积极，认为其是一个优雅且高效的对话AI框架。Parlant的社区和支持系统也为开发者提供了丰富的资源和帮助，确保他们能够顺利构建出高效的AI代理。


https://github.com/DataExpert-io/data-engineer-handbook

该项目是一个数据工程学习资源的汇总库，旨在帮助用户成为优秀的数据工程师。对于新手，建议从2024年数据工程入门路线图开始学习，此外还有为期六周的免费YouTube训练营，提供软件需求和介绍等信息。项目中包含多个实用学习资源，包括项目示例、面试建议、高质量书籍推荐、社区列表以及通过电子邮件学习的通讯。

在书籍推荐中，列出了超过25本优秀书籍，其中《数据工程基础》、《设计数据密集型应用》和《设计机器学习系统》被认为是必读书籍。社区方面，推荐加入多个数据工程和机器学习相关的社区，如DataExpert.io和Data Talks Club等。项目还列出了许多知名公司及其产品，涵盖数据编排、数据湖、数据仓库、数据质量等多个领域。

此外，项目提供了多种数据工程相关的博客、白皮书和社交媒体账号，帮助用户获取最新的行业动态和技术信息。用户还可以通过各种播客和通讯获取更多的学习资源，涵盖数据工程的各个方面。最后，项目还提供了多个在线课程和认证课程的链接，帮助用户系统性地学习数据工程知识并获得相关认证。整体而言，该项目为希望进入数据工程领域的学习者提供了全面而丰富的资源。


https://github.com/rasbt/LLMs-from-scratch

该项目旨在从零开始使用PyTorch实现一个类似于ChatGPT的大型语言模型（LLM），并提供了详细的步骤和代码。该代码库是书籍《从零开始构建大型语言模型》的官方代码库，书中通过清晰的文本、图示和示例，逐步指导读者创建自己的LLM，帮助理解大型语言模型的内部工作原理。书中介绍的方法适用于教育目的，能够创建一个小型但功能齐全的模型，并且包含了加载更大预训练模型权重以进行微调的代码。

项目的代码可以通过GitHub下载，支持在常规笔记本电脑上运行，无需特殊硬件，且能够自动利用可用的GPU。书中还提供了与每章内容相对应的练习和解决方案，帮助读者巩固所学知识。此外，书中附带了一个17小时的伴随视频课程，内容与书籍结构一致，适合用作独立学习或配合书籍的资源。

在学习本书之前，读者需要具备扎实的Python编程基础，了解深度神经网络的基本概念将有助于理解LLM的构建。书中使用PyTorch实现代码，虽然不要求读者具备PyTorch的高级技能，但熟悉其基本概念将非常有帮助。附录部分提供了PyTorch的简要介绍以及其他相关资源。

此外，项目还包含了一些额外的材料和实验，供有兴趣的读者深入探索。对于希望参与讨论或反馈的读者，建议通过Manning论坛或GitHub讨论区进行交流。总之，该项目为希望深入了解和构建大型语言模型的读者提供了全面的资源和指导。


https://github.com/enescingoz/awesome-n8n-templates

该项目旨在通过提供一系列n8n自动化模板来提升工作流程的自动化效率。这些模板可以快速连接用户常用的应用程序，如Gmail、Telegram、Google Drive和Slack，用户只需几次点击即可使用这些AI驱动的自动化工具，从而节省时间并提高生产力。项目的文档包含了从互联网收集的多种自动化模板，旨在帮助用户轻松发现和使用适用于不同平台和服务的现成自动化方案。

在项目中，用户可以找到多种类别的模板，包括社交媒体自动化、电子邮件管理、数据处理等。例如，用户可以使用Twitter线程翻译和发布工作流，自动提取、翻译和重写Twitter内容，帮助创作者和市场营销人员以最低的成本接触新受众。此外，还有针对Gmail的自动标签、回复草稿生成和可疑邮件分析等模板，旨在提高电子邮件管理的效率。

项目还提供了与Telegram、Google Drive、Google Sheets等多种工具的集成，用户可以通过AI助手与这些工具进行交互，自动化数据处理和内容生成。对于需要处理文档的用户，项目中也包含了PDF处理和文档分析的工作流，帮助用户从文档中提取信息并进行智能分析。

需要注意的是，所有模板均为在线收集，项目作者不对使用这些模板可能引发的问题或损失承担责任。用户可以通过参与项目，贡献新的模板或建议新的类别来丰富这一资源库。总之，该项目为希望提升工作效率的用户提供了丰富的自动化工具和解决方案。


https://github.com/PixiEditor/PixiEditor

PixiEditor是一款通用的2D编辑器，旨在为用户提供满足各种2D需求的工具和功能。用户可以利用该软件创建游戏精灵、动画、编辑图像以及设计标志，所有功能都集成在一个直观且熟悉的界面中。PixiEditor 2.0版本默认提供三种工具集，包括像素艺术工具、基本绘画工具和矢量工具，用户可以在同一画布上混合使用这些工具，并支持导出为多种格式，如png、jpg、svg、gif和mp4等。

该版本还引入了时间轴和动画功能，用户可以逐帧创建动画，或使用节点来动画自定义着色器。未来的开发计划中还包括对矢量关键帧动画的支持。PixiEditor的节点渲染系统为其强大的功能提供了支持，所有图层、效果和层结构都以节点的形式存在，用户可以自由定制图像，创造程序化艺术和动画。

此外，PixiEditor还提供了从源代码构建的指南，鼓励用户参与贡献和协作。对于遇到问题的用户，PixiEditor团队也提供了帮助和支持。总之，PixiEditor是一个功能强大且灵活的2D编辑工具，适合各种创作需求。


https://github.com/immich-app/immich

该项目是一个高性能的自托管照片和视频管理解决方案，旨在为用户提供便捷的多媒体管理体验。项目目前处于非常活跃的开发阶段，用户在使用时需注意可能存在的错误和重大变更，因此不建议将该应用作为唯一的照片和视频存储方式。用户应遵循3-2-1备份策略，以确保珍贵数据的安全。

项目的主要文档和安装指南可以在官方网站找到，用户可以通过提供的链接访问相关信息，包括功能介绍、安装要求和开发路线图等。该解决方案支持多种语言，便于全球用户使用。用户还可以通过演示链接体验应用的功能，演示账户的登录信息也已提供。

该应用的功能丰富，包括支持视频和照片的上传与查看、自动备份、避免资产重复、选择性相册备份、下载本地设备、多人支持、共享相册、原始格式支持、元数据查看、基于元数据和面部识别的搜索等。移动端和网页端均可使用这些功能，部分功能如用户管理仅在网页端提供。

此外，应用还支持背景备份、OAuth认证、API密钥、LivePhoto和MotionPhoto的备份与播放、360度图像显示、用户自定义存储结构、公共分享、归档与收藏、全球地图、面部识别与聚类、记忆功能等。移动端还提供离线支持和只读画廊功能，确保用户在不同场景下的使用需求。

总之，该项目为用户提供了一个全面的照片和视频管理平台，适合希望自托管多媒体内容的用户，但在使用时需谨慎对待数据安全和备份策略。


https://github.com/Shubhamsaboo/awesome-llm-apps

该项目是一个汇集了众多优秀大型语言模型（LLM）应用的集合，主要利用了OpenAI、Anthropic、Gemini等公司的模型以及一些开源模型，如DeepSeek、Qwen和Llama。项目中包含了多种应用，涵盖了AI代理、检索增强生成（RAG）、多代理团队、语音代理等多种形式，用户可以在本地计算机上运行这些应用。

该集合的目的在于展示LLM在不同领域的实际和创造性应用，用户可以探索如何将这些模型与AI代理结合使用，提升工作效率和创造力。此外，项目提供了详细的文档，鼓励用户学习和参与开源生态系统的建设。项目中列出了多个特色AI项目，包括初级和高级AI代理、自动游戏代理、多代理团队、语音AI代理等，涵盖了从数据分析到医疗影像、旅行规划等多个应用场景。

用户可以通过克隆项目仓库、安装依赖并按照项目特定说明进行设置和运行应用。项目还欢迎社区的贡献，鼓励用户提出改进建议或新增应用，并要求遵循现有的项目结构，提供详细的文档说明。总之，该项目不仅为用户提供了丰富的LLM应用资源，也为开发者提供了一个参与和贡献的机会，推动了AI技术的进一步发展。


https://github.com/MotiaDev/motia

Motia是一个现代化的后端框架，旨在解决当前后端开发中的碎片化问题。传统的后端开发通常涉及多个框架和工具，例如API、后台作业、队列和AI代理等，导致开发过程复杂且难以维护。Motia通过将这些功能统一到一个系统中，提供了共享的可观察性和开发者体验，简化了后端开发的流程。开发者可以在同一个工作流中使用JavaScript、TypeScript、Python等多种语言，极大地提高了开发效率。

Motia的核心概念是“步骤”（Step），每个步骤代表一个独立的入口点，如API、后台作业、定时任务和AI代理等。通过这种方式，开发者可以轻松地在同一项目中使用不同的编程语言，同时共享状态。Motia还提供了内置的可观察性工具，支持完整的端到端追踪、结构化日志记录和状态可视化，帮助开发者快速调试和监控应用。

使用Motia，开发者可以在不到60秒的时间内启动一个新项目，并通过简单的命令创建REST API、处理后台任务和调度作业。Motia的工作台提供了可视化界面，方便开发者实时构建、测试和观察后端应用。它还支持事件驱动架构，使得复杂的工作流可以通过简单的声明式代码构建，避免了基础设施的复杂性。

Motia的设计理念是消除后端开发中的多语言障碍和可观察性缺口，提供一个统一的运行时环境，简化部署和扩展过程。通过Motia，开发者可以专注于业务逻辑，而不必担心基础设施的配置和管理。总之，Motia为现代后端开发提供了一个高效、灵活且易于使用的解决方案。


https://github.com/OpenBB-finance/OpenBB

OpenBB平台是一个开源的金融数据聚合器，旨在为用户和人工智能代理提供多种金融数据访问，包括股票、期权、加密货币、外汇、宏观经济和固定收益等。用户可以通过简单的Python命令安装并使用该平台，获取所需的金融数据。OpenBB还提供了丰富的扩展功能，以满足不同用户的需求。

该平台的接口支持Python和命令行界面（CLI），用户可以通过OpenBB工作区实现数据可视化和AI代理的集成。用户只需按照简单的步骤将OpenBB平台与工作区连接，即可开始使用。安装过程也非常简便，用户可以通过PyPI包或直接克隆代码库来完成。

OpenBB鼓励社区参与，用户可以通过提交GitHub问题、提供反馈或直接贡献代码来参与项目。平台的使用涉及一定的风险，用户在进行金融交易前应充分了解相关风险和成本，并根据自身的投资目标和风险承受能力做出决策。

此外，OpenBB还提供了多种联系方式，用户可以通过电子邮件或社交媒体与团队联系，获取支持或讨论合作机会。总之，OpenBB平台致力于通过开放的金融数据访问和社区参与，推动金融行业的创新与发展。


## Repo Trendings

### Repo: coleam00/Archon

url: https://github.com/coleam00/Archon

language: Python


Beta release of Archon OS - the knowledge and task management backbone for AI coding assistants.


### Repo: emcie-co/parlant

url: https://github.com/emcie-co/parlant

language: Python


LLM agents built for control. Designed for real-world use. Deployed in minutes.


### Repo: DataExpert-io/data-engineer-handbook

url: https://github.com/DataExpert-io/data-engineer-handbook

language: Jupyter Notebook


This is a repo with links to everything you'd ever want to learn about data engineering


### Repo: rasbt/LLMs-from-scratch

url: https://github.com/rasbt/LLMs-from-scratch

language: Jupyter Notebook


Implement a ChatGPT-like LLM in PyTorch from scratch, step by step


### Repo: enescingoz/awesome-n8n-templates

url: https://github.com/enescingoz/awesome-n8n-templates

language: Unknown


Supercharge your workflow automation with this curated collection of n8n templates! Instantly connect your favorite apps-like Gmail, Telegram, Google Drive, Slack, and more-with ready-to-use, AI-powered automations. Save time, boost productivity, and unlock the true potential of n8n in just a few clicks.


### Repo: PixiEditor/PixiEditor

url: https://github.com/PixiEditor/PixiEditor

language: C#


PixiEditor is a Universal Editor for all your 2D needs


### Repo: immich-app/immich

url: https://github.com/immich-app/immich

language: TypeScript


High performance self-hosted photo and video management solution.


### Repo: Shubhamsaboo/awesome-llm-apps

url: https://github.com/Shubhamsaboo/awesome-llm-apps

language: Python


Collection of awesome LLM apps with AI Agents and RAG using OpenAI, Anthropic, Gemini and opensource models.


### Repo: MotiaDev/motia

url: https://github.com/MotiaDev/motia

language: TypeScript


Modern Backend Framework that unifies APIs, background jobs, workflows, and AI agents into a single cohesive system with built-in observability and state management.


### Repo: OpenBB-finance/OpenBB

url: https://github.com/OpenBB-finance/OpenBB

language: Python


Financial data aggregator for humans and AI agents.


## Paper Trendings

### Paper: We-Math 2.0: A Versatile MathBook System for Incentivizing Visual   Mathematical Reasoning

url: http://arxiv.org/pdf/2508.10433v1


Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across various tasks, but still struggle with complex mathematical reasoning. Existing research primarily focuses on dataset construction and method optimization, often overlooking two critical aspects: comprehensive knowledge-driven design and model-centric data space modeling. In this paper, we introduce We-Math 2.0, a unified system that integrates a structured mathematical knowledge system, model-centric data space modeling, and a reinforcement learning (RL)-based training paradigm to comprehensively enhance the mathematical reasoning abilities of MLLMs. The key contributions of We-Math 2.0 are fourfold: (1) MathBook Knowledge System: We construct a five-level hierarchical system encompassing 491 knowledge points and 1,819 fundamental principles. (2) MathBook-Standard & Pro: We develop MathBook-Standard, a dataset that ensures broad conceptual coverage and flexibility through dual expansion. Additionally, we define a three-dimensional difficulty space and generate 7 progressive variants per problem to build MathBook-Pro, a challenging dataset for robust training. (3) MathBook-RL: We propose a two-stage RL framework comprising: (i) Cold-Start Fine-tuning, which aligns the model with knowledge-oriented chain-of-thought reasoning; and (ii) Progressive Alignment RL, leveraging average-reward learning and dynamic data scheduling to achieve progressive alignment across difficulty levels. (4) MathBookEval: We introduce a comprehensive benchmark covering all 491 knowledge points with diverse reasoning step distributions. Experimental results show that MathBook-RL performs competitively with existing baselines on four widely-used benchmarks and achieves strong results on MathBookEval, suggesting promising generalization in mathematical reasoning.


### Paper: NextStep-1: Toward Autoregressive Image Generation with Continuous   Tokens at Scale

url: http://arxiv.org/pdf/2508.10711v1


Prevailing autoregressive (AR) models for text-to-image generation either rely on heavy, computationally-intensive diffusion models to process continuous image tokens, or employ vector quantization (VQ) to obtain discrete tokens with quantization loss. In this paper, we push the autoregressive paradigm forward with NextStep-1, a 14B autoregressive model paired with a 157M flow matching head, training on discrete text tokens and continuous image tokens with next-token prediction objectives. NextStep-1 achieves state-of-the-art performance for autoregressive models in text-to-image generation tasks, exhibiting strong capabilities in high-fidelity image synthesis. Furthermore, our method shows strong performance in image editing, highlighting the power and versatility of our unified approach. To facilitate open research, we will release our code and models to the community.


### Paper: ToonComposer: Streamlining Cartoon Production with Generative   Post-Keyframing

url: http://arxiv.org/pdf/2508.10881v1


Traditional cartoon and anime production involves keyframing, inbetweening, and colorization stages, which require intensive manual effort. Despite recent advances in AI, existing methods often handle these stages separately, leading to error accumulation and artifacts. For instance, inbetweening approaches struggle with large motions, while colorization methods require dense per-frame sketches. To address this, we introduce ToonComposer, a generative model that unifies inbetweening and colorization into a single post-keyframing stage. ToonComposer employs a sparse sketch injection mechanism to provide precise control using keyframe sketches. Additionally, it uses a cartoon adaptation method with the spatial low-rank adapter to tailor a modern video foundation model to the cartoon domain while keeping its temporal prior intact. Requiring as few as a single sketch and a colored reference frame, ToonComposer excels with sparse inputs, while also supporting multiple sketches at any temporal location for more precise motion control. This dual capability reduces manual workload and improves flexibility, empowering artists in real-world scenarios. To evaluate our model, we further created PKBench, a benchmark featuring human-drawn sketches that simulate real-world use cases. Our evaluation demonstrates that ToonComposer outperforms existing methods in visual quality, motion consistency, and production efficiency, offering a superior and more flexible solution for AI-assisted cartoon production.


### Paper: PRELUDE: A Benchmark Designed to Require Global Comprehension and   Reasoning over Long Contexts

url: http://arxiv.org/pdf/2508.09848v2


We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning.


### Paper: UI-Venus Technical Report: Building High-performance UI Agents with RFT

url: http://arxiv.org/pdf/2508.10833v2


We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venus's summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models. To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies. To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment & Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/inclusionAI/UI-Venus.


### Paper: Puppeteer: Rig and Animate Your 3D Models

url: http://arxiv.org/pdf/2508.10898v1


Modern interactive applications increasingly demand dynamic 3D content, yet the transformation of static 3D models into animated assets constitutes a significant bottleneck in content creation pipelines. While recent advances in generative AI have revolutionized static 3D model creation, rigging and animation continue to depend heavily on expert intervention. We present Puppeteer, a comprehensive framework that addresses both automatic rigging and animation for diverse 3D objects. Our system first predicts plausible skeletal structures via an auto-regressive transformer that introduces a joint-based tokenization strategy for compact representation and a hierarchical ordering methodology with stochastic perturbation that enhances bidirectional learning capabilities. It then infers skinning weights via an attention-based architecture incorporating topology-aware joint attention that explicitly encodes inter-joint relationships based on skeletal graph distances. Finally, we complement these rigging advances with a differentiable optimization-based animation pipeline that generates stable, high-fidelity animations while being computationally more efficient than existing approaches. Extensive evaluations across multiple benchmarks demonstrate that our method significantly outperforms state-of-the-art techniques in both skeletal prediction accuracy and skinning quality. The system robustly processes diverse 3D content, ranging from professionally designed game assets to AI-generated shapes, producing temporally coherent animations that eliminate the jittering issues common in existing methods.


### Paper: STream3R: Scalable Sequential 3D Reconstruction with Causal Transformer

url: http://arxiv.org/pdf/2508.10893v1


We present STream3R, a novel approach to 3D reconstruction that reformulates pointmap prediction as a decoder-only Transformer problem. Existing state-of-the-art methods for multi-view reconstruction either depend on expensive global optimization or rely on simplistic memory mechanisms that scale poorly with sequence length. In contrast, STream3R introduces an streaming framework that processes image sequences efficiently using causal attention, inspired by advances in modern language modeling. By learning geometric priors from large-scale 3D datasets, STream3R generalizes well to diverse and challenging scenarios, including dynamic scenes where traditional methods often fail. Extensive experiments show that our method consistently outperforms prior work across both static and dynamic scene benchmarks. Moreover, STream3R is inherently compatible with LLM-style training infrastructure, enabling efficient large-scale pretraining and fine-tuning for various downstream 3D tasks. Our results underscore the potential of causal Transformer models for online 3D perception, paving the way for real-time 3D understanding in streaming environments. More details can be found in our project page: https://nirvanalan.github.io/projects/stream3r.


### Paper: A Survey on Diffusion Language Models

url: http://arxiv.org/pdf/2508.10875v1


Diffusion Language Models (DLMs) are rapidly emerging as a powerful and promising alternative to the dominant autoregressive (AR) paradigm. By generating tokens in parallel through an iterative denoising process, DLMs possess inherent advantages in reducing inference latency and capturing bidirectional context, thereby enabling fine-grained control over the generation process. While achieving a several-fold speed-up, recent advancements have allowed DLMs to show performance comparable to their autoregressive counterparts, making them a compelling choice for various natural language processing tasks. In this survey, we provide a holistic overview of the current DLM landscape. We trace its evolution and relationship with other paradigms, such as autoregressive and masked language models, and cover both foundational principles and state-of-the-art models. Our work offers an up-to-date, comprehensive taxonomy and an in-depth analysis of current techniques, from pre-training strategies to advanced post-training methods. Another contribution of this survey is a thorough review of DLM inference strategies and optimizations, including improvements in decoding parallelism, caching mechanisms, and generation quality. We also highlight the latest approaches to multimodal extensions of DLMs and delineate their applications across various practical scenarios. Furthermore, our discussion addresses the limitations and challenges of DLMs, including efficiency, long-sequence handling, and infrastructure requirements, while outlining future research directions to sustain progress in this rapidly evolving field. Project GitHub is available at https://github.com/VILA-Lab/Awesome-DLMs.


### Paper: Pass@k Training for Adaptively Balancing Exploration and Exploitation of   Large Reasoning Models

url: http://arxiv.org/pdf/2508.10751v1


Reinforcement learning with verifiable rewards (RLVR), which typically adopts Pass@1 as the reward, has faced the issues in balancing exploration and exploitation, causing policies to prefer conservative actions, converging to a local optimum. Identifying an appropriate reward metric is therefore crucial. Regarding the prior work, although Pass@k has been used in evaluation, its connection to LLM exploration ability in RLVR remains largely overlooked. To investigate this, we first use Pass@k as the reward to train the policy model (i.e., $\textbf{Pass@k Training}$), and observe the improvement on its exploration ability. Next, we derive an analytical solution for the advantage of Pass@k Training, leading to an efficient and effective process. Building on this, our analysis reveals that exploration and exploitation are not inherently conflicting objectives, while they can mutually enhance each other. Moreover, Pass@k Training with analytical derivation essentially involves directly designing the advantage function. Inspired by this, we preliminarily explore the advantage design for RLVR, showing promising results and highlighting a potential future direction.


### Paper: HumanSense: From Multimodal Perception to Empathetic Context-Aware   Responses through Reasoning MLLMs

url: http://arxiv.org/pdf/2508.10576v1


While Multimodal Large Language Models (MLLMs) show immense promise for achieving truly human-like interactions, progress is hindered by the lack of fine-grained evaluation frameworks for human-centered scenarios, encompassing both the understanding of complex human intentions and the provision of empathetic, context-aware responses. Here we introduce HumanSense, a comprehensive benchmark designed to evaluate the human-centered perception and interaction capabilities of MLLMs, with a particular focus on deep understanding of extended multimodal contexts and the formulation of rational feedback. Our evaluation reveals that leading MLLMs still have considerable room for improvement, particularly for advanced interaction-oriented tasks. Supplementing visual input with audio and text information yields substantial improvements, and Omni-modal models show advantages on these tasks. Furthermore, we argue that appropriate feedback stems from a contextual analysis of the interlocutor's needs and emotions, with reasoning ability serving as the key to unlocking it. Accordingly, we employ a multi-stage, modality-progressive reinforcement learning to enhance the reasoning abilities of an Omni model, achieving substantial gains on evaluation results. Additionally, we observe that successful reasoning processes exhibit highly consistent thought patterns. By designing corresponding prompts, we also enhance the performance of non-reasoning models in a training-free manner. Project page: \textcolor{brightpink}https://digital-avatar.github.io/ai/HumanSense/


### Paper: Processing and acquisition traces in visual encoders: What does CLIP   know about your camera?

url: http://arxiv.org/pdf/2508.10637v1


Prior work has analyzed the robustness of visual encoders to image transformations and corruptions, particularly in cases where such alterations are not seen during training. When this occurs, they introduce a form of distribution shift at test time, often leading to performance degradation. The primary focus has been on severe corruptions that, when applied aggressively, distort useful signals necessary for accurate semantic predictions.   We take a different perspective by analyzing parameters of the image acquisition process and transformations that may be subtle or even imperceptible to the human eye. We find that such parameters are systematically encoded in the learned visual representations and can be easily recovered. More strikingly, their presence can have a profound impact, either positively or negatively, on semantic predictions. This effect depends on whether there is a strong correlation or anti-correlation between semantic labels and these acquisition-based or processing-based labels. Our code and data are available at: https://github.com/ryan-caesar-ramos/visual-encoder-traces


### Paper: From Black Box to Transparency: Enhancing Automated Interpreting   Assessment with Explainable AI in College Classrooms

url: http://arxiv.org/pdf/2508.10860v1


Recent advancements in machine learning have spurred growing interests in automated interpreting quality assessment. Nevertheless, existing research suffers from insufficient examination of language use quality, unsatisfactory modeling effectiveness due to data scarcity and imbalance, and a lack of efforts to explain model predictions. To address these gaps, we propose a multi-dimensional modeling framework that integrates feature engineering, data augmentation, and explainable machine learning. This approach prioritizes explainability over ``black box'' predictions by utilizing only construct-relevant, transparent features and conducting Shapley Value (SHAP) analysis. Our results demonstrate strong predictive performance on a novel English-Chinese consecutive interpreting dataset, identifying BLEURT and CometKiwi scores to be the strongest predictive features for fidelity, pause-related features for fluency, and Chinese-specific phraseological diversity metrics for language use. Overall, by placing particular emphasis on explainability, we present a scalable, reliable, and transparent alternative to traditional human evaluation, facilitating the provision of detailed diagnostic feedback for learners and supporting self-regulated learning advantages not afforded by automated scores in isolation.


### Paper: When Explainability Meets Privacy: An Investigation at the Intersection   of Post-hoc Explainability and Differential Privacy in the Context of Natural   Language Processing

url: http://arxiv.org/pdf/2508.10482v2


In the study of trustworthy Natural Language Processing (NLP), a number of important research fields have emerged, including that of explainability and privacy. While research interest in both explainable and privacy-preserving NLP has increased considerably in recent years, there remains a lack of investigation at the intersection of the two. This leaves a considerable gap in understanding of whether achieving both explainability and privacy is possible, or whether the two are at odds with each other. In this work, we conduct an empirical investigation into the privacy-explainability trade-off in the context of NLP, guided by the popular overarching methods of Differential Privacy (DP) and Post-hoc Explainability. Our findings include a view into the intricate relationship between privacy and explainability, which is formed by a number of factors, including the nature of the downstream task and choice of the text privatization and explainability method. In this, we highlight the potential for privacy and explainability to co-exist, and we summarize our findings in a collection of practical recommendations for future work at this important intersection.


